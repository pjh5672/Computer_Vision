{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accf2698",
   "metadata": {},
   "source": [
    "### Classifying images with DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c126ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18b641f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to C:\\Users\\PARK JIHO/.cache\\torch\\hub\\main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to C:\\Users\\PARK JIHO/.cache\\torch\\hub\\checkpoints\\deit_base_patch16_224-b5f2ef4d.pth\n",
      "66.6%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "72.0%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "78.7%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "86.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "93.4%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100.0%\n",
      "C:\\Users\\PARK JIHO\\anaconda3\\envs\\pro_1\\lib\\site-packages\\torchvision\\transforms\\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
    "img = transform(img)[None,]\n",
    "out = model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec42baf",
   "metadata": {},
   "source": [
    "### Scripting DeiT\n",
    "To use the model on mobile, we first need to script the model. See the Script and Optimize recipe for a quick overview. Run the code below to convert the DeiT model used in the previous step to the TorchScript format that can run on mobile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24894167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PARK JIHO/.cache\\torch\\hub\\facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"fbdeit_scripted.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8cded",
   "metadata": {},
   "source": [
    "### Quantizing DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3644e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'fbgemm' for server inference and 'qnnpack' for mobile inference\n",
    "backend = \"fbgemm\" # replaced with qnnpack causing much worse inference speed for quantized model on this notebook\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "scripted_quantized_model.save(\"fbdeit_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0da21f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceddd60",
   "metadata": {},
   "source": [
    "### Optimizing DeiT\n",
    "The final step before using the quantized and scripted model on mobile is to optimize it:\n",
    "The generated fbdeit_optimized_scripted_quantized.pt file has about the same size as the quantized, scripted, but non-optimized model. The inference result remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36cf373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
    "optimized_scripted_quantized_model.save(\"fbdeit_optimized_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "686ae964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PARK JIHO\\anaconda3\\envs\\pro_1\\lib\\site-packages\\torch\\nn\\modules\\module.py:889: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:930.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "out = optimized_scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea6e9e3",
   "metadata": {},
   "source": [
    "### Using Lite Interpreter\n",
    "Although the lite model size is comparable to the non-lite version, when running the lite version on mobile, the inference speed up is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c556a2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_scripted_quantized_model._save_for_lite_interpreter(\"fbdeit_optimized_scripted_quantized_lite.ptl\")\n",
    "ptl = torch.jit.load(\"fbdeit_optimized_scripted_quantized_lite.ptl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c69df",
   "metadata": {},
   "source": [
    "### Comparing Inference Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad6d8f88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model: 434.83ms\n",
      "scripted model: 431.19ms\n",
      "scripted & quantized model: 189.13ms\n",
      "scripted & quantized & optimized model: 261.31ms\n",
      "lite model: 242.90ms\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
    "    out = model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
    "    out = scripted_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
    "    out = scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
    "    out = optimized_scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
    "    out = ptl(img)\n",
    "\n",
    "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
    "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
    "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02cd26c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Model Inference Time Reduction\n",
      "0                          original model       434.83ms        0%\n",
      "1                          scripted model       431.19ms     0.84%\n",
      "2              scripted & quantized model       189.13ms    56.50%\n",
      "3  scripted & quantized & optimized model       261.31ms    39.91%\n",
      "4                              lite model       242.90ms    44.14%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n",
    "df = pd.concat([df, pd.DataFrame([\n",
    "    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
    "    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n",
    "    columns=['Inference Time', 'Reduction'])], axis=1)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03714fee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
