{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "from VOC_dataset import VOCDataset\n",
    "from config import DefaultConfig\n",
    "from fcos import FCOS, DetectHead \n",
    "from loss import coords_fmap2orig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "root_dir = './test_images/'\n",
    "image_list = [os.path.join(root_dir, fn) for fn in os.listdir(root_dir) if fn.endswith('.jpg')]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "source": [
    "config = DefaultConfig\n",
    "\n",
    "class FCOSDetector(nn.Module):\n",
    "    def __init__(self,mode=\"training\",config=None):\n",
    "        super().__init__()\n",
    "        self.mode=mode\n",
    "        self.fcos_body=FCOS(config=config)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        batch_imgs = inputs\n",
    "        out=self.fcos_body(batch_imgs)\n",
    "        return out\n",
    "    \n",
    "ckpt = torch.load(\"./models/voc2012_512x800_epoch100_loss0.6055.pth\",map_location=torch.device('cpu'))\n",
    "\n",
    "model=FCOSDetector(mode=\"inference\")\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "print('')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO===>success frozen BN\n",
      "INFO===>success frozen backbone stage1\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "source": [
    "resize = transforms.Resize((512))\n",
    "to_tensor = transforms.ToTensor()\n",
    "to_normalize = transforms.Normalize((0.485,0.456,0.406), (0.229,0.224,0.225))\n",
    "\n",
    "idx = 0\n",
    "image = Image.open(image_list[idx]).convert('RGB')\n",
    "tensor_img = to_normalize(to_tensor(resize(image)))\n",
    "tensor_img = tensor_img.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(tensor_img)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### DetectHead"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "source": [
    "def _reshape_cat_out(inputs,strides):\n",
    "    batch_size=inputs[0].shape[0]\n",
    "    c=inputs[0].shape[1]\n",
    "    out=[]\n",
    "    coords=[]\n",
    "    for pred,stride in zip(inputs,strides):\n",
    "        pred=pred.permute(0,2,3,1)\n",
    "        coord=coords_fmap2orig(pred,stride).to(device=pred.device)\n",
    "        pred=torch.reshape(pred,[batch_size,-1,c])\n",
    "        out.append(pred)\n",
    "        coords.append(coord)\n",
    "    return torch.cat(out,dim=1),torch.cat(coords,dim=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "def _coords2boxes(coords, offsets):\n",
    "    x1y1=coords[None,:,:]-offsets[...,:2]\n",
    "    x2y2=coords[None,:,:]+offsets[...,2:] #[batch_size,sum(_h*_w),2]\n",
    "    boxes=torch.cat([x1y1,x2y2],dim=-1) #[batch_size,sum(_h*_w),4]\n",
    "    return boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def _post_process(self,preds_topk):\n",
    "    _cls_scores_post=[]\n",
    "    _cls_classes_post=[]\n",
    "    _boxes_post=[]\n",
    "    cls_scores_topk,cls_classes_topk,boxes_topk=preds_topk\n",
    "    \n",
    "    for batch in range(cls_classes_topk.shape[0]):\n",
    "        mask=cls_scores_topk[batch]>=score_threshold\n",
    "        _cls_scores_b=cls_scores_topk[batch][mask]\n",
    "        _cls_classes_b=cls_classes_topk[batch][mask]\n",
    "        _boxes_b=boxes_topk[batch][mask]\n",
    "        nms_ind=self.batched_nms(_boxes_b,_cls_scores_b,_cls_classes_b,self.nms_iou_threshold)\n",
    "        _cls_scores_post.append(_cls_scores_b[nms_ind])\n",
    "        _cls_classes_post.append(_cls_classes_b[nms_ind])\n",
    "        _boxes_post.append(_boxes_b[nms_ind])\n",
    "    scores,classes,boxes= torch.stack(_cls_scores_post,dim=0),torch.stack(_cls_classes_post,dim=0),torch.stack(_boxes_post,dim=0)\n",
    "\n",
    "    return scores,classes,boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    " def box_nms(boxes,scores,thr):\n",
    "        if boxes.shape[0]==0:\n",
    "            return torch.zeros(0,device=boxes.device).long()\n",
    "        assert boxes.shape[-1]==4\n",
    "        x1,y1,x2,y2=boxes[:,0],boxes[:,1],boxes[:,2],boxes[:,3]\n",
    "        areas=(x2-x1+1)*(y2-y1+1)\n",
    "        order=scores.sort(0,descending=True)[1]\n",
    "        keep=[]\n",
    "        while order.numel()>0:\n",
    "            if order.numel()==1:\n",
    "                i=order.item()\n",
    "                keep.append(i)\n",
    "                break\n",
    "            else:\n",
    "                i=order[0].item()\n",
    "                keep.append(i)\n",
    "            \n",
    "            xmin=x1[order[1:]].clamp(min=float(x1[i]))\n",
    "            ymin=y1[order[1:]].clamp(min=float(y1[i]))\n",
    "            xmax=x2[order[1:]].clamp(max=float(x2[i]))\n",
    "            ymax=y2[order[1:]].clamp(max=float(y2[i]))\n",
    "            inter=(xmax-xmin).clamp(min=0)*(ymax-ymin).clamp(min=0)\n",
    "            iou=inter/(areas[i]+areas[order[1:]]-inter)\n",
    "            idx=(iou<=thr).nonzero().squeeze()\n",
    "            if idx.numel()==0:\n",
    "                break\n",
    "            order=order[idx+1]\n",
    "        return torch.LongTensor(keep)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "def batched_nms(boxes, scores, idxs, iou_threshold):\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty((0,), dtype=torch.int64, device=boxes.device)\n",
    "    # strategy: in order to perform NMS independently per class.\n",
    "    # we add an offset to all the boxes. The offset is dependent\n",
    "    # only on the class idx, and is large enough so that boxes\n",
    "    # from different classes do not overlap\n",
    "    max_coordinate = boxes.max()\n",
    "    offsets = idxs.to(boxes) * (max_coordinate + 1)\n",
    "    boxes_for_nms = boxes + offsets[:, None]\n",
    "    keep = box_nms(boxes_for_nms, scores, iou_threshold)\n",
    "    return keep"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "source": [
    "def _post_process(preds_topk):\r\n",
    "    _cls_scores_post=[]\r\n",
    "    _cls_classes_post=[]\r\n",
    "    _boxes_post=[]\r\n",
    "    cls_scores_topk,cls_classes_topk,boxes_topk=preds_topk\r\n",
    "    \r\n",
    "    for batch in range(cls_classes_topk.shape[0]):\r\n",
    "        mask=cls_scores_topk[batch]>=score_threshold\r\n",
    "        _cls_scores_b=cls_scores_topk[batch][mask]#[?]\r\n",
    "        _cls_classes_b=cls_classes_topk[batch][mask]#[?]\r\n",
    "        _boxes_b=boxes_topk[batch][mask]#[?,4]\r\n",
    "        nms_ind=batched_nms(_boxes_b,_cls_scores_b,_cls_classes_b,nms_iou_threshold)\r\n",
    "        \r\n",
    "        _cls_scores_post.append(_cls_scores_b[nms_ind])\r\n",
    "        _cls_classes_post.append(_cls_classes_b[nms_ind])\r\n",
    "        _boxes_post.append(_boxes_b[nms_ind])\r\n",
    "        \r\n",
    "    scores,classes,boxes= torch.stack(_cls_scores_post,dim=0),torch.stack(_cls_classes_post,dim=0),torch.stack(_boxes_post,dim=0)\r\n",
    "    return scores,classes,boxes"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "# detection_head=DetectHead(config.score_threshold,config.nms_iou_threshold,config.max_detection_boxes_num,config.strides,config)\r\n",
    "score_threshold=config.score_threshold\r\n",
    "nms_iou_threshold=config.nms_iou_threshold\r\n",
    "max_detection_boxes_num=config.max_detection_boxes_num\r\n",
    "strides=config.strides\r\n",
    "\r\n",
    "inputs = out\r\n",
    "\r\n",
    "cls_logits,coords=_reshape_cat_out(inputs[0], strides) #[batch_size,sum(_h*_w),class_num]\r\n",
    "cnt_logits,_=_reshape_cat_out(inputs[1], strides) #[batch_size,sum(_h*_w),1]\r\n",
    "reg_preds,_=_reshape_cat_out(inputs[2], strides) #[batch_size,sum(_h*_w),4]\r\n",
    "\r\n",
    "cls_preds=cls_logits.sigmoid_()\r\n",
    "cnt_preds=cnt_logits.sigmoid_()\r\n",
    "\r\n",
    "cls_scores,cls_classes=torch.max(cls_preds,dim=-1) #[batch_size,sum(_h*_w)]\r\n",
    "cls_scores=cls_scores*(cnt_preds.squeeze(dim=-1)) #[batch_size,sum(_h*_w)]\r\n",
    "cls_classes=cls_classes+1 #[batch_size,sum(_h*_w)]\r\n",
    "\r\n",
    "boxes=_coords2boxes(coords, reg_preds)\r\n",
    "\r\n",
    "# select top k\r\n",
    "max_num = min(max_detection_boxes_num, cls_scores.shape[-1])\r\n",
    "topk_ind = torch.topk(cls_scores, max_num, dim=-1, largest=True, sorted=True)[1]\r\n",
    "\r\n",
    "_cls_scores=[]\r\n",
    "_cls_classes=[]\r\n",
    "_boxes=[]\r\n",
    "for batch in range(cls_scores.shape[0]):\r\n",
    "    _cls_scores.append(cls_scores[batch][topk_ind[batch]])#[max_num]\r\n",
    "    _cls_classes.append(cls_classes[batch][topk_ind[batch]])#[max_num]\r\n",
    "    _boxes.append(boxes[batch][topk_ind[batch]])#[max_num,4]\r\n",
    "    \r\n",
    "cls_scores_topk = torch.stack(_cls_scores,dim=0)#[batch_size,max_num]\r\n",
    "cls_classes_topk = torch.stack(_cls_classes,dim=0)#[batch_size,max_num]\r\n",
    "boxes_topk = torch.stack(_boxes,dim=0)#[batch_size,max_num,4]\r\n",
    "\r\n",
    "scores,classes,boxes = _post_process([cls_scores_topk,cls_classes_topk,boxes_topk])\r\n",
    "\r\n",
    "batch_boxes=boxes.clamp_(min=0)\r\n",
    "h,w=tensor_img.shape[2:]\r\n",
    "batch_boxes[...,[0,2]]=batch_boxes[...,[0,2]].clamp_(max=w-1)\r\n",
    "batch_boxes[...,[1,3]]=batch_boxes[...,[1,3]].clamp_(max=h-1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "image = image.resize((tensor_img.shape[3], tensor_img.shape[2]))\n",
    "cv_img = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "boxes=boxes[0].cpu().numpy().tolist()\n",
    "classes=classes[0].cpu().numpy().tolist()\n",
    "scores=scores[0].cpu().numpy().tolist()\n",
    "\n",
    "for i, box in enumerate(boxes):\n",
    "    pt1 = (int(box[0]), int(box[1]))\n",
    "    pt2 = (int(box[2]), int(box[3]))\n",
    "    cv2.rectangle(cv_img,pt1,pt2,(0,255,0))\n",
    "    cv2.putText(cv_img,\"%s %.3f\"%(VOCDataset.CLASSES_NAME[int(classes[i])],scores[i]),(int(box[0]+5),int(box[1])+15),cv2.FONT_HERSHEY_SIMPLEX,0.5,[0,200,20],2)\n",
    "\n",
    "cv2.imshow('', cv_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}